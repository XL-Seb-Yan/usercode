%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2006/03/15
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallextended]{svjour3}     % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}         % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
\journalname{JournalofGridComputing}
%
\begin{document}

\title{Distributed Analysis in CMS}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

\author{Alessandra Fanfani \and Andrea Sciab`a \and Anzar Afaq \and Carlos Kavka \and Chih-Hao Huang \and Christoph Wissing \and Claudio Grandi \and Daniele Bonacorsi \and Daniele Spiga \and Daniel Riley \and Dave Evans \and Eric Vaandering \and Fabio Farina \and Federica Fanzago \and Frank Wuerthwein \and Giuseppe Codispoti \and Hassen Riahi \and Ian Fisk \and Ilaria Villella \and James Letts \and Jorgen D'Hondt \and Joris Maes \and Jose' M. Hern´andez \and Joseph Flix \and Jukka Klem \and Julia Andreeva \and Marco Calloni \and Mattia Cinquilli \and Niccolo' Magini \and Pablo Saiz \and Petra Van Mulders \and Ricky Egeland \and Sanjay Padhi \and Simon Metson  \and Stefano Belforte \and Stefano Lacaprara \and Thomas Kress \and Tony Wildish \and Valentin Kuznetsov \and Vijay Sekhri \and Vincenzo Miccio \and Yuyi Guo }

\authorrunning{ Alessandra Fanfani \and Andrea Sciab`a \and Anzar Afaq \and Carlos Kavka \and Chih-Hao Huang \and Chris Brew \and Christoph Wissing \and Claudio Grandi \and Daniele Bonacorsi \and Daniele Spiga \and Daniel Riley \and Dave Evans \and David Colling \and Eric Vaandering \and Fabio Farina \and Federica Fanzago \and Frank Wuerthwein \and Gerhild Maier \and Giuseppe Bagliesi \and Giuseppe Codispoti \and Hassen Riahi \and Ian Fisk \and Ilaria Villella \and James Letts \and Jorgen D'Hondt \and Joris Maes \and Jose' M. Hern´andez \and Joseph Flix \and Jukka Klem \and Julia Andreeva \and Ken Bloom \and Marco Calloni \and Mattia Cinquilli \and Niccolo' Magini \and Pablo Saiz \and Peter Kreuzer \and Petra Van Mulders \and Ricky Egeland \and Sanjay Padhi \and Simon Metson  \and Stefano Belforte \and Stefano Lacaprara \and Subir Sarkar \and Thomas Kress \and Tony Wildish \and Valentin Kuznetsov \and Vijay Sekhri \and Vincenzo Miccio \and Yuyi Guo} % if too long for running head

\institute{A. Fanfani \and D. Bonacorsi \and G. Codispoti \and C. Grandi
           \at INFN and University of Bologna, viale Berti Pichat 6/2, 40127 Bologna, Italy \\
              Tel.: +39-51-2095232 Fax: +39-51-209XXXX\\
              \email{fanfani@bo.infn.it}
%             \emph{Present address:} of F. Author  %  if needed
           \and 
            M. Calloni \and N. Magini \and D. Spiga \and J. Andreeva \and P. Saiz \and V. Miccio \at CERN, Switzerland
           \and
           S. Lacaprara \at Legnaro INFN, Italy
           \and
           F. Fanzago \at Padova INFN, Italy
           \and 
           S. Belforte \and C. Kavka \at Trieste INFN, Italy
           \and
           F. Farina \at Milano Bicocca INFN, Italy
           \and
           M. Cinquilli \and H. Riahi \at Perugia INFN, Italy
           \and
           S. Metson \at Bristol University, UK
           \and
           C. Wissing \at DESY,  Germany
           T. Kress \at RWTH , Germany
           \and
           J. D'Hondt \and J.Maes \and P. Van Mulders \and I. Villella \at Brussel University, Belgium
           \and
           J. Klem \at Helsinki Institute of Physics, Finland
           \and
            J. M. Hern´andez \at CIEMAT, Madrid, Spain
           \and
            J. Flix \at PIC, Barcelona, Spain
           \and
           A. Afaq \and D. Evans \and I. Fisk \and Y. Guo \and C. Huang \and V. Sekhri \and E.Vaandering \at Fermilab, Batavia, Illinois, USA
           \and 
           V. Kuznetsov \and D. Riley \at Cornell University, Ithaca, New York, USA 
           \and 
           R. Egeland \at Minnesota University, Twin Cities, USA
           \and 
           T. Wildish \at Princeton University, USA
           \and
           J. Letts \and S. Padhi \and F. Wuerthwein \at S. Diego University, USA
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla blabla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla blabla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla blabla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla bla
\keywords{LHC \and CMS \and Distributed Analysis}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}
Bla bla bla bla bla CMS\cite{RefCMS}.
A large experimental community, more
than 3000 collaborators, distributed over more than 38 countries all around the world .....

%\paragraph{Paragraph headings} Use paragraph headings as needed.
\section{The CMS Computing Model}
\label{sec:2}
The CMS distributed computing and analysis model is designed to serve, process and archive the %huge amount of data the experiment will collect.
%large amounts of events that will be available when the detector starts collecting data.
large number of events that will be generated when the CMS detector starts taking data. The computing resources are geographically distributed, interconnected via high throughput networks and operated by means of Grid software. 
%%  motivations to choose a distributed environment
The choice of a distributed system allows to delegate responsibilities to local CMS communities, access to additional funding channels and ensure load balancing of the available resources while replicating the interesting data in different sites.


A multi-Tier hierarchical distributed model is adopted in CMS with specific functionality at different levels.
\subsection{Tier-0}
\label{sec:2_1}
The Tier-0 centre at CERN accepts data from the CMS online system, archives the data, performs prompt first pass reconstruction. Reconstructed data at the Tier-0 together with the corresponding raw data are distributed to Tier-1s over the optical private network. In addition to the Tier-0 centre, CERN hosts the CMS Analysis Facility (CAF) that is focused on latency-critical detector, trigger and calibration activities.
Roughly 20\% of the computing capacity is located at CERN, while the remaining is distributed.

\subsection{Tier-1}
\label{sec:2_2}
The Tier-1 are 7 large regional computing centers located in France, Germany, Italy, Spain, Taiwan, United Kingdom and United States, where organized mass data processing are performed. That includes re-processing, data skimming and other organized intensive analysis tasks. 
%From CM CHEP09: For reconstruction a nominal Tier-1 should be capable of processing 70Hz worth of data with a required IO rate of 100MB/s. Skimming through the reconstructed datasets uses only 1\% of CPU of reconstruction so sparse reading of the file is required to have manageable IO rates.
The Tier-1 centres archive the fraction of data distributed to them as well as the simulated data produced at the Tier-2 centres,  while they are serving analysis transfer requests to Tier-2.
%From CM CHEP09: In the CMS experiment the Tier-1 centers rather than the Tier-0 serve the majority of the data to the Tier-2 centers for analysis.

\subsection{Tier-2}
\label{sec:2_3}
Tier-2 centers provide capacity for user data analysis and for production of simulated data.
In CMS data transfer to Tier-2 can occur from any Tier-1. A significant effort is required in 
commissioning all possible transfer links, as described in section ~\ref{sec:4_1_4}, as well
as improving site availability, as described in section ~\ref{sec:4_1_3}.

\section{Framework for CMS Distributed Analysis}
\label{sec:3}
The CMS analysis model foresees activites driven by data location. Data are scattered over many computing centers according to CMS data placement policies and user analysis runs where data is located. In order to enable distributed analysis a set of Workload and Data Management tools have been designed, building CMS-specific services on top of existing Grid ones.

\subsection{Data Management}
\label{sec:3_1}
The CMS Data Management System provides the basic infrastructure and tools necessary to manage the large amounts of data produced, processed and analysed in a distributed computing environment. It is made of a set of loosely coupled components as described in the following sections.
% I'm negletting local catalogue....
\subsubsection{DBS}
\label{sec:3_1_1}
The Dataset Bookkeeping Service (DBS)\cite{RefDBS} provides the means to describe, discover and use CMS event data. 
It catalogs CMS specific data definitions such as run number, the algorithms and configurations used to process the data toghether with the information regarding the processing parentage of the data it describes.
The DBS stores information about CMS data in a queryable format. The supported queries allow to discover available data and how they are organized logically in term of packaging units like files and groups of files. The information available from queries to DBS are site independent.

The DBS is used for data discovery and job configuration by the production and analysis system through a DBS API.
Users can discover what data exists using a Web browser or Command Line Interface.
The DBS is usable in multiple “scopes”:
\begin{itemize}
\item A Global scope DBS is a single instance describing data CMS-wide %all data used by the experiment
\item Many local scopes are established to describe data produced by MonteCarlo production, Physics groups or individuals. Data described in local scope will eventually be migrated to the global scope. 
\end{itemize}
The DBS system is a multi-tier web application whose design is modular and makes it easily adaptable to multiple database technologies. The supported types of database (ORACLE, MySQL and SQLite) enable the DBS deployment in a range of environments from general CMS at large installations to specific personal installations.
An XML format is used for the format of the HTTP payload exchanged with the client.

The Global DBS is hosted at CERN and its database engine is the CERN Oracle RAC (Real Application Cluster) server for CMS. Some local scopes DBS instances that catalogs Physics groups data are also hosted at CERN. There are also DBS instances installed at other sites for private use. 
% If possible find out the amount of data (datasets,files) described so far in Global DBS

\subsubsection{PhEDEx}
\label{sec:3_1_2}
The CMS data placement and transfer systems are implemented by PhEDEx\cite{RefPhEDEx}. The data placement system provides an interface to define, execute and monitor administrative decision of data movement like where experiment data is to be located, which copies are custodial, etc. 
Data are distributed according to available resources and physics interests at sites. 
% Users can request data transfer to a site however an authorization by the site data manager is required
The data transfer system manages the replication of files from multiple sources to multiple destinations. It relies on transfer nodes dealing with %site specific replication and tape management. 
the underlying grid file and storage management services.
PhEDEx keeps track of data location in the distributed computing system. The analysis system refers to PhEDEx to obtain the locations of the data.

Technically PhEDEx is based on software agents storing their state and communicating via a central “black board” database hosted in the CERN Oracle RAC. A set of service agents are operated centrally at CERN while each site in general operates only the agents that interact with the storage at the site. A web site offers various management tools and allows to monitor to current and historical transfer conditions.

\emph{FIXME: fill phedex usage numbers} In the last XY months PhEDEx transferred XY PB data. In months XXX the average global daily transfer rate was XYTB/day or XY GB/s. 
%PhEDEx has demonstrated capacity to sustain well over xy million transfers an hour for extended periods of time.

\subsection{Workload Management}
some introduction....
\label{sec:3_2}
\subsubsection{CRAB}
The CMS Remote Analysis Builder (CRAB)\cite{RefCRAB} have been developed as a user-friendly interface to handle data analysis in a distributed environment, hiding the complexity of interactions to the Grid and CMS services.
It allow the user to run over large distributed data samples the very same 
analysis code he has developed locally in a small scale test. 
The functionalities CRAB provides, as schematically illustrated in Fig.~\ref{fig:CRABWorkflow}, are:
\begin{itemize}
\item{Data discovery and location:}
Facilitate queries of the experiment data catalogues (DBS and PhEDEx) to find which data and where is to be accessed.
\item{Job preparation:}
Pack local user code and the environment to be sent to remote sites where the CMS software is pre-installed as described in ~\ref{sec:4_1_1}.
\item{Job splitting:}
Decide how to configure each job to access a subset of files in the dataset to effectively use the Tier-2s resources.
\item{Job submission:}
Submit to Grid sites hosting the user required data.
\item{Job monitoring:}
Monitor the status of the submitted jobs querying Grid services.
\item{Handling Output data:}
Copy the produced output to a remote Tier-2 the user is associated with or return it to the user for small files (few MB).
Publish the produced data with their description and provenance into a local DBS so that the data can be used in further analysis and shared with other colleagues.
\end{itemize} 

%% Grid integration via BOSSLite
CRAB is implemented in python. The interface to the Grid middlewares and local batch systems is provided by a python library named BOSSLite\cite{RefBOSSLite}. It relies on a database to track and log information about the user requested task into an entity-relation schema.
An abstract interface is used to access the database through safe sessions and connection pools to grant safe operation in a multiprocessing/multi threaded environment. Current implementation supports MySQL and SQLite databases.
Standard operations such as job submission, tracking, cancel and output retrieval are also performed via a generic abstract interface. Scheduler specific interfaces are implemented in specific plug-ins, loaded at run-time. Presently, plug-ins are implemented for Grid middleware EGEE (gLite) \cite{RefgLiteWMS}, OSG \cite{RefOSG} and ARC (NorduGrid)\cite{RefARC} and for LSF and SGE batch systems.

%\emph{FIXME: are few lines about grid specific implementation required? Such as the following lines about gLite}
%Using the gLite WMS BOSSLite enables bulk job submission through gLite collections. This allows a faster job submission, but also an optimized job status tracking through bulk queries. Since the CMS computing model uses its own data location system, the gLite WMS match making has the main role to perform a load balancing among sites that are hosting the data to be accessed. 

%% CRAB server motivation and architecture:
The interaction with the Grid can be either direct with a thin CRAB client or using an intermediate CRAB Analysis Server~\cite{RefCRAB} (see Fig.~\ref{fig:CRABWorkflow}). The CRAB Analysis Server automates the user analysis workflow with resubmissions, error handling, output retrieval and leaving to the user just the preparation of the configuration file and notifying him of the output availability. In addition it has the capability of implementing advanced analysis use cases.
The Analysis Server is made of a set of independent components communicating asynchronously through a shared messaging service and cooperating to carry out the analysis workflow. The communication between client and server is implemented using the gSOAP framework and Grid credentials of users are delegated to server.
The Analysis Server is coupled with an external GridFTp server %Storage Element
 that stores the user input and output data, allowing to implement CMS policies on sandbox sizes, bypassing for instance the gLite WMS limits.


\begin{figure}
 \includegraphics[width=0.70\textwidth]{figures/CRABWorkflow.png}
\caption{CRAB workflow}
\label{fig:CRABWorkflow}
\end{figure}

\subsection{Monitoring}
\label{sec:3_3}
\subsubsection{Dashboard}
\emph{TO BE FILLED BY STEFANOB}

\section{Operation of CMS Distributed Analysis}
\label{sec:4}
\subsection{Distributed Infrastructure}
\label{sec:4_1}
\emph{Short overview, grid Middleware needed, central grid services}

\subsubsection{ SiteDB }
%% From DanieleB
SiteDB is a catalogue of all CMS computing Tiers. It records the CMS resources at the site, the resources pledged for the future, and it keeps track of CMS personnel at each site, including the roles and duties they fulfull in the collaboration.
%Collaborators are automatically added to the SiteDB database on registering with the CMS Hypernews system. As well as providing the aforementioned information, SiteDB acts as a simple monitoring portal, bringing  in and providing links to content from PhEDEx, DBS, SAM tests, CMS Dashboard, GOCDB and GSTAT. The database also maintains a mapping between the different names a site can be known by, for instance the institute name (University of Bristol), its SAM/WLCG name (UKI-SOUTHGRID-BRIS-HEP), and the CMS name (T2_UK_SGrid_Bristol), as well as associations between sites (e.g. the parent T1 site for a T2)
CMS has designed and built SiteDB because CMS relies on close contact to the sites it uses. The distributed nature of CMS computing makes it very useful to track the people's responsbilities, and to contact them on the basis of their role. Some site roles are technical (for instance running PhEDEx) others are related to CMS computing policy (e.g. the site's Data Manager). 
%For users (both end users, and consumer of the info like the computing shifters) who have a problem with a site and may not know what the various roles and responsibilities are at a site, SiteDB provides this information. 
In additions, SiteDB links to the CMS Savannah Computing Infrastructure portal, one of the tools CMS uses for internal  tracking of problems across the distributed infrastructure. In Savannah there is a group ('squad' in Savannah jargon) per site, dynamically maintained from the information in SiteDB via a set of cron-tabbed scripts.

\subsubsection{ Software Installation }
\label{sec:4_1_1}
Distributed analysis relies on the experiment software being pre-installed on the remote
sites for each release and removed when declared obsolete. 
The installation procedure is triggered centrally via high priority Grid jobs, which also 
publish the versions available on a site information system.

\label{sec:4_1_2}
\subsubsection{ Site Readiness }
\label{sec:4_1_3} 
%systematically test each site and improve the availability of the Tier-1s and Tier-2 sites
\emph{USE THE COMMISSIONING PAPER AT CHEP09 CR2009-089}

\emph{USE site commissioning section in INTEGRATION PAPER AT CHEP09 CR2009-087}

\subsubsection{ Link Commissioning }
%% From DanieleB
An ad-hoc task force (Debugging Data Transfers,DDT)\cite{RefDDT} was created to coordinate the debugging of data transfer links, in order to commission most crucial transfer routes among CMS Tiers by designing and enforcing a clear procedure to debug problematic links. The DDT procedures are now part of the commissioned links overview.
\emph{USE DDT PAPER AT CHEP09 CR2009-093}


\label{sec:4_1_4}
\subsection{Analysis Support at CMS Sites}
\label{sec:4_2}
\subsubsection{ Organized Analysis Activities }
\label{sec:4_2_1}
Skimming, t1access role 
\emph{TO BE FILLED BY CLAUDIO}

\subsubsection{ User and Physics Group Support }
\label{sec:4_2_2}
Association T2-PAG
Storage Hierarchy

USE THE COMPUTING MODEL PAPER AT CHEP09 CR2009-084
%Making efficient use of the networking and ensuring the appropriate revision of a dataset is at a location with sufficient processing resources is a challenging data management exercise. The knowledge of the needs of many
%groups of people is aggregated in the group leaders and not in an automated software system. CMS has attempted to increase the number of people empowered to make data management decisions by dividing the available storage into chunks. The 200TB of Tier-2 storage is divided into 4 logical pieces. At the storage at the Tier-2s increases in 2009 and beyond the allocations to groups and central space will grow.
%1. Local Group and User Space: This is roughly 30TB per group currently with an additional
%1TB per user that is controlled and managed by the geographically local community.
%2. Physics Group Space: 60-90TB of space is allocated to 2-3 physics analysis groups.
%Representatives from the groups serve as data managers for the space and make
%subscription and deletion requests. The space for each group will increase with time as datasets grow.
%3. Centrally Controlled Space: 30TB (growing to 50TB in 2009) of space is identified at each
%Tier-2 under the control of CMS centrally. This is used to ensure complete copies of the
%reconstruction datasets are available across the Tier-2s.
%4. Simulation Staging and Temporary Space: 20TB is identified to stage simulation produced at the Tier-2s and other temporary files before they can either be merged or transferred to the permanent home.
%CMS is in the process of training users and administrators to manage the storage at the Tier-2s to efficiently support analysis. Even in the current commissioning phase CMS has hundreds of active users and more than 5PB of disk space across the Tier-2s. We expect to utilize the space
%dynamically and take advantage of the wide area network links to update samples frequently.
%Shifting to this paradigm involves solving technical challenges as well as engaging a large community of users.

\subsection{Experience with CMS Distributed Analysis}
\label{sec:4_3}
\subsubsection{Tests}
\label{sec:4_3_1}
During the Common Computing Readiness Challenge (CCRC08) in May 2008
the CRAB Server was used to simulate physics group activities
submitting realistic user analysis jobs lasting approximately 4
hours. In the CMS computing model the datasets are replicated at
several sites and each user has space at a given site to store his
analysis outputs. To mimic the analysis scenario the kind of jobs used
in the challenge were reading datasets available at all sites and
producing an output file that was staged-out remotely to a single
site~\cite{Refsites}. More than 100,000 jobs on about 30 sites were
submitted in two weeks and the CRAB Server provided the needed
functionality for job submission and tracking. It is worth noting that
the aim of this challenge was to test the readiness of sites, by
filling their resources, rather than a scale test of the CRAB Server.

In order to test the actual server scalability and reliability up to
the expected CMS operational rates a dedicated test environment was
set up. The Storage Element, based on GridFTP, that CRAB server relies
on was installed on a different machine with respect to the machine
hosting the CRAB Server components. The aim was to decouple the load
due the shipping of input/output sandboxes and the core workflow
management. The machine hosting the Storage Element and the CRAB
server were both 4 CPU 2000 MHz Dual Core AMD Opterons with 4 GB
RAM. The test was performed in the gLite context using two dedicated
gLite WMS. Monitoring information was collected from various sources
like the CRAB Server database tracking the job flow and the CPU usage
of its components, as well as from underlying services like MySQL and
GridFTP server and gLite WMS monitoring~\cite{wmsMon}. The kind of
jobs submitted were very short jobs running for less than a few
minutes, not reading a dataset and without stage-out. This choice was
made to provide an harsher environment for job handling due to higher
rate of finished jobs and to limit the resource usage at sites.
The test was split into two phases: a controlled submission pattern
originated by a single user and an emulation of a multi-user
environment.

\begin{itemize}
\item{   Single user phase}

A single user performed a controlled job submission pattern: peaks of
1000-2000 jobs every 5 hours superimposed on a constant rate of
500-600 jobs every 20 minutes. Almost 200,000 jobs were submitted to
more than 50 sites with a rate above 40,000 jobs/day, as shown in
Figure \ref{fig:stresssingle}. The CRAB Server was able to cope with that
rate with no indication of reaching a breaking point. No significant
backlogs were accumulated. The main contributors to the load of the
system were the MySQL and GridFTP usage, using about 2 and 1.5 CPUs
\begin{figure}
\includegraphics[width=0.55\textwidth]{figures/SingleUserJobStatus.png}
%\includegraphics[width=3.5in]{SingleUserJobStatus.png}
\caption{Cumulative distribution of jobs submitted to CRAB Server
  during the single user test phase. Aborted jobs were Grid related
  problems at some sites. }
\label{stresssingle}
\end{figure}

\item{    Multi user environment phase}

In the second testing phase the job submissions were from different
user certificates, thus emulating the CRAB Server usage by more
users. The need to further stress the multi-user environment arises
from its higher complexity. Single threads of the very same component
may be handling different user's jobs, requiring different
certificates. Any interaction among them may cause failures, stopping
the component work and causing a backlog.

Different submission patterns were adopted by the 12 users
involved. For example a user submitting 100 jobs every 15 minutes,
another 500 jobs every 20 minutes, another 2000 jobs every 6 hours
etc. plus a couple of users submitting randomly at their will.  No
CRAB Server misbehaviour was identified due to the multi-user
environment. With respect to the single-user test the submission rate
was increased to above 50,000 jobs/day and this helped identify some
limitations in the communication between JobTracking and GetOutput
components that caused a backlog of jobs without output
retrieved. This effect is more evident for homogeneous very short
jobs, as those used in the test, that have a higher finishing rate
than real user's jobs which tend to finish at different times. None
the less, the backlog was absorbed in a few hours. This issue was
taken into account in the development cycle and the code was
optimized.  About 120,000 jobs were successfully handled in 48 hours,
as shown in Figure \ref{fig:stressmulti}.  The CPU load due to MySQL
proved to be stable regardless of the database size increase with more
jobs in the system. Overall the breakdown of CPU load usage is 2~CPUs
for MySQL, about 1.5~CPUs for GridFTP and about 1~CPU for all the CRAB
Server components, thus outlining the need of at least a 4~CPU
machine.  The load due to GridFTP is such that it's not compulsory to
have the GridFTP server decoupled from the machine hosting the CRAB
Server components.

Currently the whole CMS analysis is made of 30,000 jobs/day and the
CMS Computing model target is around 100,000-200,000 jobs/day. Some
CRAB Server instances deployed at different sites to serve physic's
group activities and regional community, as foreseen, can cope with
analysis needs.
\end{itemize}
\begin{figure}
\centering
\includegraphics[width=0.55\textwidth]{figures/MultiUserJobStatus.png}
\caption{Cumulative distribution of jobs submitted to CRAB Server
  during the multi-user test phase. }
\label{stressmulti}
\end{figure}



\subsubsection{Sustained Analysis}
\label{sec:4_3_2}
Mention that distributed analysis is done for CRAFT analysis and MC analysis, ...

CRAB Analysis Server instances have been deployed in several countries (CERN, Italy, France).  
A couple of them are open to worldwide distributed CMS users. Other instances are being used by local communities or specific physics group.

Describe the number of WMS used.

Number of users, Number of jobs, statistics (EGEE/OSG, WMS/Condor-G, ...), failures, look
at James' metrics
USE PADA ASTF CHEP09 PAPER CR2009-088

During last year about 11 million analysis jobs were submitted.  Peaks
of more than 100,000 jobs per day have been reached, with an average
of 30,000 jobs per day, as shown in Figure~\ref{fig:jobs}.  About 78\% of
the CMS analysis jobs were submitted using gLite WMS.  Since the gLite
architecture is such that the system scales linearly with the number
of gLite WMSs used, analysis jobs are balanced currently over 7
WMS. The rest of the analysis jobs are submitted using Condor-G.
\begin{figure}
\includegraphics[width=3.5in]{figures/crabjobsdaily.png}
\caption{Number of daily jobs terminated in 2008. }
\label{fig:jobs}
\end{figure}
\begin{figure}
\includegraphics[width=3.5in]{figures/UserInteg.png}
\caption{Cumulative number of distinct CRAB users starting from 2008. }
\label{fig:intuser}
\end{figure}
\begin{figure}
\includegraphics[width=3.5in]{figures/crabusersdaily.png}
\caption{Number of CRAB daily users in the 2008. }
\label{fig:distusers}
\end{figure}

As shown in Figure~\ref{fig:intuser}, since 2008 the number of distinct
CRAB users has grown to more than 1000, almost 30\% of the whole CMS
community.  About 100 users per day use CRAB to submit analysis jobs
over simulated and cosmic ray samples, as shown in
Figure~\ref{fig:distusers}.
Analysis job efficiency is slightly above 60\%. Most of the failures
faced by the users are due to user application errors, remote stageout
issues and errors reading data at the site. Application failures are
expected, since analysis jobs run user code which may not have been
thoroughly tested.  Failures in stage out of the data output files to
remote storage can be due to users misconfiguring the remote
destination to use or to transfer problems.  Failures accessing data
at the site mainly expose problems with the storage at the site or
inconsistencies between the data catalogue and what has been deleted at
the site.  A few percent of failures are due to jobs aborted by the
Grid, however these are often due to site problems or jobs that spend
too much time on the worker node and are killed by the local batch
system, appearing as aborted by the Grid.
% From Jan09 to May09:
% overall analysis job efficiency 62%
% breakdown of failures:
% - Grid failures
%   8% Grid Aborted jobs
% - Application errors:
%   35% remote stageout
%   22% CMSSW error (8001 , 8009) --> I suspect that some of the 8001
%                                      are due to error reading input files
%   13% error reading files at site (8020)
%    6.8% error 50115


\section{Conclusions}
\label{sec:5}

% For one-column wide figures use
%\begin{figure}
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
%  \includegraphics{cations, both on the client-side and on the
%% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:1}       % Give a unique label
%\end{figure}
%


%% For tables use
%\begin{table}
% table caption is above the table
%\caption{Please write your table caption here}
%\label{tab:1}       % Give a unique label
% For LaTeX tables use
%\begin{tabular}{lll}
%\hline\noalign{\smallskip}
%first & second & third  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%number & number & number \\
%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}

%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

% Non-BibTeX users please use
\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
% Format for Journal Reference
\bibitem{RefCMS}
CMS Collaboration R. Adolphi et al., The CMS experiment at the CERN LHC, JINST, 0803, S08004 (2008)
%
\bibitem{RefCM}
CMS Collaboration, Computing Model TDR....
%
\bibitem{RefDBS}
A. Afaq et al., The CMS dataset bookkeeping service, J.Phys.Conf.Ser, 119, 072001 (2008)
%
\bibitem{RefPhEDEx}
L. Tuura et al., Scaling CMS data transfer system for LHC start-up, J.Phys.Conf.Ser, 119, 072030 (2008)
%
\bibitem{RefCRAB}
D. Spiga et al., The CMS Remote Analysis Builder CRAB, 14th Int. Conf. on High Performance Computing ISBN/ISSN: 978-3-540-77219-4 , 4873, 580-586 (2007)\\
G. Codispoti et al., CRAB: a CMS Application for Distributed Analysis, to be published in TNS...
%
\bibitem{RefWLCG}
LCG Computing Grid Technical Design Report, LCG-TDR-001 CERN/LHCC 2005-024 (2005)
%
\bibitem{RefBOSSLite}
G. Codispoti et al., Use of the gLite-WMS in CMS for production and analysis, Proceedings of International Conference On Computing In High Energy Physics And Nuclear Physics (2009). \emph{IS IT POSSIBLE TO REFERNCE NOT YET PUBLISHED CHEP09 PAPER?}
%
\bibitem{RefgLiteWMS} P. Andreetto et al., The gLite Workload Management System, J.Phys.Conf.Ser, 119, 062007 (2008)
%
\bibitem{RefOSG} R. Pordes et al, The Open Science Grid, J.Phys.Conf.Ser, 78, 012057 (2007)
%
\bibitem{RefARC} M.Ellertet al., Advanced Resource Connector middleware
  for lightweight computational Grids, Future Generation Computer Systems, 23, 219-240 (2007)
%
\bibitem{RefDDT} N. Magini et al., The CMS Data Transfer Test Environment in Preparation for LHC Data Taking, Proceedings of NSS-IEEE, Dresden (2008)  \emph{ARE THERE MORE DETAILED INFO ABOUT THIS RERENCE?}
%% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
\end{thebibliography}

\end{document}
% end of file template.tex

